{"./":{"url":"./","title":"前言","keywords":"","body":"HTML解析库Python版jQuery：PyQuery 最新版本：v1.0 更新时间：20210414 简介 介绍HTML解析领域的Python版的jQuery：PyQuery。先对PyQuery概述，再介绍常见的基本操作，包括如何获取元素的文本值，如何获取带子元素的元素的文本值；获取元素的属性，如何用CSS选择器去选择定位查找元素；对比了find和items的返回结果；整理了CSS选择器的基本语法；给出了具体的实际例子，比如用PySpider抓取汽车之家的品牌和车型车系数据中如何使用PyQuery。 源码+浏览+下载 本书的各种源码、在线浏览地址、多种格式文件下载如下： Gitbook源码 crifan/python_html_parse_pyquery: HTML解析库Python版jQuery：PyQuery 如何使用此Gitbook源码去生成发布为电子书 详见：crifan/gitbook_template: demo how to use crifan gitbook template and demo 在线浏览 HTML解析库Python版jQuery：PyQuery book.crifan.com HTML解析库Python版jQuery：PyQuery crifan.github.io 离线下载阅读 HTML解析库Python版jQuery：PyQuery PDF HTML解析库Python版jQuery：PyQuery ePub HTML解析库Python版jQuery：PyQuery Mobi 版权说明 此电子书教程的全部内容，如无特别说明，均为本人原创和整理。其中部分内容参考自网络，均已备注了出处。如有发现侵犯您版权，请通过邮箱联系我 admin 艾特 crifan.com，我会尽快删除。谢谢合作。 鸣谢 感谢我的老婆陈雪的包容理解和悉心照料，才使得我crifan有更多精力去专注技术专研和整理归纳出这些电子书和技术教程，特此鸣谢。 更多其他电子书 本人crifan还写了其他100+本电子书教程，感兴趣可移步至： crifan/crifan_ebook_readme: Crifan的电子书的使用说明 crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2021-04-14 19:50:55 "},"overview/":{"url":"overview/","title":"PyQuery概述","keywords":"","body":"PyQuery概述 概述 作用 Python版的jQuery 注：jQuery是JavaScript中的库，可以（很方便的）操作HTML（中的元素） 使用 PySpider内部也用到了PyQuery 官网文档 入口 https://pythonhosted.org/pyquery/index.html Quickstart = 快速开始 https://pythonhosted.org/pyquery/index.html#quickstart Attributes = 元素属性 https://pythonhosted.org/pyquery/attributes.html CSS = 用CSS选择器选择元素 https://pythonhosted.org/pyquery/css.html Manipulating = 操作元素 https://pythonhosted.org/pyquery/manipulating.html Traversing = 元素遍历 https://pythonhosted.org/pyquery/traversing.html API https://pythonhosted.org/pyquery/api.html Scraping = 抓取（页面） https://pythonhosted.org/pyquery/scrap.html pyquery.ajax – PyQuery AJAX extension = AJAX扩展 https://pythonhosted.org/pyquery/ajax.html Tips = 提示 https://pythonhosted.org/pyquery/tips.html Testing = 测试 https://pythonhosted.org/pyquery/testing.html crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2021-04-14 19:36:05 "},"common_operation/":{"url":"common_operation/","title":"PyQuery常见操作","keywords":"","body":"PyQuery常见操作 获取元素的文本值 举例：html By Judy Love , Julie Danneberg 写法： curHtmlElement.text() 举例： authors = [] for eachAuthor in response.doc('p[class=\"contributors\"] a[href] strong').items(): print(\"eachAuthor=%s\", eachAuthor) authorText = eachAuthor.text() 详见官网文档： https://pythonhosted.org/pyquery/api.html#pyquery.pyquery.PyQuery.text 获取含有子元素的a的本身的text 页面： html 30周年年型 Sportback 35 TFSI 进取型 (131张) 想要：获取a的本身的text 代码： for eachLiAElt in ddUlEltList[curIdx].items(\"li a\"): curModelName = eachLiAElt.contents()[0] 输出值：30周年年型 Sportback 35 TFSI 进取型 效果： 官网文档 pyquery.pyquery.PyQuery.contents Return contents (with text nodes) 和之前用过的BeautifulSoup的contents是类似的 BeautifulSoup contents tag的 .contents 属性可以将tag的子节点以列表的方式输出 warning:: 直接用text()会包含子元素的text值 如果直接用text()： for eachLiAElt in ddUlEltList[curIdx].items(\"li a\"): curModelName = eachLiAElt.text() 是错误的写法。 因为会把span中的text，此处的(131张)也包括在内 获取元素的属性的值 语法： attr htmlElement.attr(\"property name\") 如果property name是连接在一起的单词这种，比如xxx, xxx_yyy，那么可以写成： htmlElement.attr.property_name 举例1 GUIDED READING LEXILE® MEASURE Grade Level Equivalent DRA LEVEL 对应代码： readingLevelSelectElement = readingLevelElement.find('select[id=\"book-detail-reading-level\"]') print(\"readingLevelSelectElement=%s\" % readingLevelSelectElement) guidedReading = readingLevelSelectElement.find('option[value=\"GRL\"]').attr(\"data-reading-level\") lexileMeasure = readingLevelSelectElement.find('option[value=\"LEX\"]').attr(\"data-reading-level\") gradeLevelEquivalent = readingLevelSelectElement.find('option[value=\"GLE\"]').attr(\"data-reading-level\") draLevel = readingLevelSelectElement.find('option[value=\"DRA\"]').attr(\"data-reading-level\") print(\"guidedReading=%s\" % guidedReading) print(\"lexileMeasure=%s\" % lexileMeasure) print(\"gradeLevelEquivalent=%s\" % gradeLevelEquivalent) print(\"draLevel=%s\" % draLevel) 即可提取到内容： readingLevelSelectElement= GUIDED READING LEXILE® MEASURE Grade Level Equivalent DRA LEVEL guidedReading=J lexileMeasure=240L gradeLevelEquivalent=None draLevel=16 举例2 页面： html: 厂商指导价：14.59万元 想要：获取span的data-price的属性的值 正确写法 msrpPriceStr = msrpPriceElt.attr(\"data-price\") 错误写法： msrpPriceStr = msrpPriceElt.attr.data-price warning:: 不是用PyQuery.val去获取元素属性值 注意此处不是 pyquery – PyQuery complete API — pyquery 1.2.4 documentation 中的： PyQuery.val(value=) 去获取属性的值的 CSS选择器的写法 attribute = value类的元素定位 例子1 调试看到的 html代码是： PySpider内部会自动加上当前host，即http前缀的http://xxx，实际上是： (PySpider中的)PyQuery的写法： response.doc('ul[id^=\"list-user\"] a[href^=\"http\"]') 或： response.doc('ul[id^=\"list-user\"] li a[href^=\"http\"]') （PySpider中的）效果：可以找到元素 例子2 页面： 您的浏览器不支持Video标签。 想要：获取src值 代码： # videoUrl = response.doc('video source[src$=\".mp4\"]') videoUrl = response.doc('video source[src^=\"http\"]').attr(\"src\") nth-child类的元素定位 页面： 徐欣蕊 热度：65.00 想要定位：div下面第二个span css选择器的写法： div[class=\"v-user\"] span:nth-child(2) crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2021-04-14 19:49:46 "},"common_operation/find_vs_items.html":{"url":"common_operation/find_vs_items.html","title":"find() vs items()","keywords":"","body":"find() vs items() 概述 PyQuery.find()：返回的是lxml的element的generator PyQuery.items()：返回的是PyQuery的generator 详解 find 官网文档：pyquery.pyquery.PyQuery.find PyQuery.find(selector) Find elements using selector traversing down from self: >> m = 'Whoah! there' >> d = PyQuery(m) >> d('p').find('em') [, ] >> d('p').eq(1).find('em') [] -> find()返回的是element元素=lxml.html.HtmlElement的generator items 官网文档：pyquery.pyquery.PyQuery.items PyQuery.items(selector=None) Iter over elements. Return PyQuery objects: >> d = PyQuery('foobar') >> [i.text() for i in d.items('span')] ['foo', 'bar'] >> [i.text() for i in d('span').items()] ['foo', 'bar'] -> items()返回的是PyQuery=pyquery.pyquery.PyQuery的generator 举例 ... ... find() 如果是find()： carSeriesDocGenerator = merchantRankDoc.find(\"li[id*='s']\") print(\"type(carSeriesDocGenerator)=%s\" % type(carSeriesDocGenerator)) 则返回的是PyQuery type(carSeriesDocGenerator)= 然后generator转换成list后： carSeriesDocList = list(carSeriesDocGenerator) for curSeriesIdx, eachCarSeriesDoc in enumerate(carSeriesDocList): print(\"type(eachCarSeriesDoc)=%s\" % type(eachCarSeriesDoc)) 每个元素是：lxml.html.HtmlElement type(eachCarSeriesDoc)= items() 如果换成items() carSeriesDocGenerator = merchantRankDoc.items(\"li[id*='s']\") print(\"type(carSeriesDocGenerator)=%s\" % type(carSeriesDocGenerator)) 则返回的是generator： type(carSeriesDocGenerator)= 然后generator转换成list后： carSeriesDocList = list(carSeriesDocGenerator) for curSeriesIdx, eachCarSeriesDoc in enumerate(carSeriesDocList): print(\"type(eachCarSeriesDoc)=%s\" % type(eachCarSeriesDoc)) 每个元素是：pyquery.pyquery.PyQuery type(eachCarSeriesDoc)= crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2021-04-14 19:49:27 "},"syntax/":{"url":"syntax/","title":"PyQuery语法","keywords":"","body":"PyQuery语法 CSS Selector=CSS选择器 PyQuery中选择=定位=查找元素的话，主要是用CSSS选择器。 此处贴出CSS选择器的常用的基本的语法： 选择器 举例 解释 .class .intro 选择class=\"intro\"的所有元素 #id #firstname 选择id=\"firstname\"的所有元素 元素相关 * * 选择所有元素 element p 选择所有元素 element,element div,p 选择所有 元素和所有 元素 element element div p 选择 元素内部的所有 元素 element>element div>p 选择父元素为 元素的所有 元素 element+element div+p 选择紧接在 元素之后的所有 元素 element1~element2 p~ul 选择前面有 元素的每个 元素 属性相关 [attribute] [target] 选择带有 target 属性所有元素 [attribute=value] [target=_blank] 选择 target=\"_blank\" 的所有元素 [attribute~=value] [title~=flower] 选择 title 属性包含单词 \"flower\" 的所有元素 [attribute|=value] [lang|=en] 选择 lang 属性值以 \"en\" 开头的所有元素 [attribute^=value] a[src^=\"https\"] 选择其 src 属性值以 \"https\" 开头的每个 元素 [attribute$=value] a[src$=\".pdf\"] 选择其 src 属性以 \".pdf\" 结尾的所有 元素 [attribute*=value] a[src*=\"abc\"] 选择其 src 属性中包含 \"abc\" 子串的每个 元素 元素的修饰？相关 :link a:link 选择所有未被访问的链接 :visited a:visited 选择所有已被访问的链接 :active a:active 选择活动链接 :hover a:hover 选择鼠标指针位于其上的链接 :focus input:focus 选择获得焦点的 input 元素 :before p:before 在每个 元素的内容之前插入内容 :after p:after 在每个 元素的内容之后插入内容 :first-letter p:first-letter 选择每个 元素的首字母 :first-line p:first-line 选择每个 元素的首行 :first-child p:first-child 选择属于父元素的第一个子元素的每个 元素 :first-of-type p:first-of-type 选择属于其父元素的首个 元素的每个 元素 :last-of-type p:last-of-type 选择属于其父元素的最后 元素的每个 元素 :only-of-type p:only-of-type 选择属于其父元素唯一的 元素的每个 元素 :only-child p:only-child 选择属于其父元素的唯一子元素的每个 元素 :nth-child(n) p:nth-child(2) 选择属于其父元素的第二个子元素的每个 元素 :nth-last-child(n) p:nth-last-child(2) 同上，从最后一个子元素开始计数 :nth-of-type(n) p:nth-of-type(2) 选择属于其父元素第二个 元素的每个 元素 :nth-last-of-type(n) p:nth-last-of-type(2) 同上，但是从最后一个子元素开始计数 :last-child p:last-child 选择属于其父元素最后一个子元素每个 元素 :lang(language) p:lang(it) 选择带有以 \"it\" 开头的 lang 属性值的每个 元素 :root :root 选择文档的根元素 :empty p:empty 选择没有子元素的每个 元素（包括文本节点） :target #news:target 选择当前活动的 #news 元素 :enabled input:enabled 选择每个启用的 元素 :disabled input:disabled 选择每个禁用的 元素 :checked input:checked 选择每个被选中的 元素 :not(selector) :not(p) 选择非 元素的每个元素 ::selection ::selection 选择被用户选取的元素部分 crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2021-04-14 19:42:30 "},"examples/":{"url":"examples/","title":"PyQuery实例","keywords":"","body":"PyQuery实例 下面记录一些实际案例，供参考。 crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2021-04-14 19:50:20 "},"examples/autohome_car.html":{"url":"examples/autohome_car.html","title":"汽车之家品牌车型车系","keywords":"","body":"汽车之家品牌车型车系 下面贴出，用PySpider爬取汽车之家的车型车系数据的代码，其中包含PyQuery相关代码，供参考。 #!/usr/bin/env python # -*- encoding: utf-8 -*- # Created on 2018-04-27 21:53:02 # Project: autohomeBrandData from pyspider.libs.base_handler import * import string import re class Handler(BaseHandler): crawl_config = { } # @every(minutes=24 * 60) def on_start(self): for eachLetter in list(string.ascii_lowercase): self.crawl(\"https://www.autohome.com.cn/grade/carhtml/%s.html\" % eachLetter, callback=self.gradCarHtmlPage) @catch_status_code_error def gradCarHtmlPage(self, response): print(\"gradCarHtmlPage: response=\", response) picSeriesItemList = response.doc('.rank-list-ul li div a[href*=\"/pic/series\"]').items() print(\"picSeriesItemList=\", picSeriesItemList) # print(\"len(picSeriesItemList)=%s\"%(len(picSeriesItemList))) for each in picSeriesItemList: self.crawl(each.attr.href, callback=self.picSeriesPage) @config(priority=2) def picSeriesPage(self, response): # &lt;a href=\"/pic/series-t/66.html\"&gt;查看停产车型&amp;nbsp;&amp;gt;&lt;/a&gt; # &lt;a class=\"ckmore\" href=\"/pic/series/588.html\"&gt;查看在售车型&amp;nbsp;&amp;gt;&lt;/a&gt; # &lt;span class=\"fn-right\"&gt;&amp;nbsp;&lt;/span&gt; fnRightPicSeries = response.doc('.search-pic-tbar .fn-right a[href*=\"/pic/series\"]') print(\"fnRightPicSeries=\", fnRightPicSeries) if fnRightPicSeries: # hrefValue = fnRightPicSeries.attr.href # print(\"hrefValue=\", hrefValue) # fullPicSeriesUrl = \"https://car.autohome.com.cn\" + hrefValue fullPicSeriesUrl = fnRightPicSeries.attr.href print(\"fullPicSeriesUrl=\", fullPicSeriesUrl) self.crawl(fullPicSeriesUrl, callback=self.picSeriesPage) # contine parse brand data aDictList = [] # for eachA in response.doc('.breadnav a[href^=\"/\"]').items(): for eachA in response.doc('.breadnav a[href*=\"/pic/\"]').items(): eachADict = { \"text\" : eachA.text(), \"href\": eachA.attr.href } print(\"eachADict=\", eachADict) aDictList.append(eachADict) print(\"aDictList=\", aDictList) mainBrandDict = aDictList[-3] subBrandDict = aDictList[-2] brandSerieDict = aDictList[-1] print(\"mainBrandDict=%s, subBrandDict=%s, brandSerieDict=%s\"%(mainBrandDict, subBrandDict, brandSerieDict)) dtTextList = [] for eachDt in response.doc(\"dl.search-pic-cardl dt\").items(): dtTextList.append(eachDt.text()) print(\"dtTextList=\", dtTextList) groupCount = len(dtTextList) print(\"groupCount=\", groupCount) for eachDt in response.doc(\"dl.search-pic-cardl dt\").items(): dtTextList.append(eachDt.text()) ddUlEltList = [] for eachDdUlElt in response.doc(\"dl.search-pic-cardl dd ul\").items(): ddUlEltList.append(eachDdUlElt) print(\"ddUlEltList=\", ddUlEltList) modelDetailDictList = [] for curIdx in range(groupCount): curGroupTitle = dtTextList[curIdx] print(\"------[%d] %s\" % (curIdx, curGroupTitle)) for eachLiAElt in ddUlEltList[curIdx].items(\"li a\"): # 1. model name # curModelName = eachLiAElt.text() curModelName = eachLiAElt.contents()[0] curModelName = curModelName.strip() print(\"curModelName=\", curModelName) curFullModelName = curGroupTitle + \" \" + curModelName print(\"curFullModelName=\", curFullModelName) # 2. model id + carSeriesId + spec url curModelId = \"\" curSeriesId = \"\" curModelSpecUrl = \"\" modelSpecUrlTemplate = \"https://www.autohome.com.cn/spec/%s/#pvareaid=2042128\" curModelPicUrl = eachLiAElt.attr.href print(\"curModelPicUrl=\", curModelPicUrl) #https://car.autohome.com.cn/pic/series-s32708/3457.html#pvareaid=2042220 foundModelSeriesId = re.search(\"pic/series-s(?P&lt;curModelId&gt;\\d+)/(?P&lt;curSeriesId&gt;\\d+)\\.html\", curModelPicUrl) print(\"foundModelSeriesId=\", foundModelSeriesId) if foundModelSeriesId: curModelId = foundModelSeriesId.group(\"curModelId\") curSeriesId = foundModelSeriesId.group(\"curSeriesId\") print(\"curModelId=%s, curSeriesId=%s\", curModelId, curSeriesId) curModelSpecUrl = (modelSpecUrlTemplate) % (curModelId) print(\"curModelSpecUrl=\", curModelSpecUrl) # 3. model status modelStatus = \"在售\" foundStopSale = eachLiAElt.find('i[class*=\"icon-stopsale\"]') if foundStopSale: modelStatus = \"停售\" else: foundWseason = eachLiAElt.find('i[class*=\"icon-wseason\"]') if foundWseason: modelStatus = \"未上市\" modelDetailDictList.append({ \"url\": curModelSpecUrl, \"车系ID\": curSeriesId, \"车型ID\": curModelId, \"车型\": curFullModelName, \"状态\": modelStatus }) print(\"modelDetailDictList=\", modelDetailDictList) allSerieDictList = [] for curIdx, eachModelDetailDict in enumerate(modelDetailDictList): curSerieDict = { \"品牌\": mainBrandDict[\"text\"], \"子品牌\": subBrandDict[\"text\"], \"车系\": brandSerieDict[\"text\"], \"车系ID\": eachModelDetailDict[\"车系ID\"], \"车型\": eachModelDetailDict[\"车型\"], \"车型ID\": eachModelDetailDict[\"车型ID\"], \"状态\": eachModelDetailDict[\"状态\"] } allSerieDictList.append(curSerieDict) # print(\"before send_message: [%d] curSerieDict=%s\" % (curIdx, curSerieDict)) # self.send_message(self.project_name, curSerieDict, url=eachModelDetailDict[\"url\"]) print(\"[%d] curSerieDict=%s\" % (curIdx, curSerieDict)) self.crawl(eachModelDetailDict[\"url\"], callback=self.carModelSpecPage, save=curSerieDict) # print(\"allSerieDictList=\", allSerieDictList) # return allSerieDictList #def on_message(self, project, msg): # print(\"on_message: msg=\", msg) # return msg @catch_status_code_error def carModelSpecPage(self, response): print(\"carModelSpecPage: response=\", response) # https://www.autohome.com.cn/spec/32708/#pvareaid=2042128 curSerieDict = response.save print(\"curSerieDict\", curSerieDict) # cityDealerPriceInt = 0 # cityDealerPriceElt = response.doc('.cardetail-infor-price #cityDealerPrice span span[class*=\"price\"]') # print(\"cityDealerPriceElt=%s\" % cityDealerPriceElt) # if cityDealerPriceElt: # cityDealerPriceFloatStr = cityDealerPriceElt.text() # print(\"cityDealerPriceFloatStr=\", cityDealerPriceFloatStr) # cityDealerPriceFloat = float(cityDealerPriceFloatStr) # print(\"cityDealerPriceFloat=\", cityDealerPriceFloat) # cityDealerPriceInt = int(cityDealerPriceFloat * 10000) # print(\"cityDealerPriceInt=\", cityDealerPriceInt) msrpPriceInt = 0 # body &gt; div.content &gt; div.row &gt; div.column.grid-16 &gt; div.cardetail.fn-clear &gt; div.cardetail-infor &gt; div.cardetail-infor-price.fn-clear &gt; ul &gt; li.li-price.fn-clear &gt; span # 厂商指导价=厂商建议零售价格=MSRP=Manufacturer's suggested retail price msrpPriceElt = response.doc('.cardetail-infor-price li[class*=\"li-price\"] span[data-price]') print(\"msrpPriceElt=\", msrpPriceElt) if msrpPriceElt: msrpPriceStr = msrpPriceElt.attr(\"data-price\") print(\"msrpPriceStr=\", msrpPriceStr) foundMsrpPrice = re.search(\"(?P&lt;msrpPrice&gt;[\\d\\.]+)万元\", msrpPriceStr) print(\"foundMsrpPrice=\", foundMsrpPrice) if foundMsrpPrice: msrpPrice = foundMsrpPrice.group(\"msrpPrice\") print(\"msrpPrice=\", msrpPrice) msrpPriceFloat = float(msrpPrice) print(\"msrpPriceFloat=\", msrpPriceFloat) msrpPriceInt = int(msrpPriceFloat * 10000) print(\"msrpPriceInt=\", msrpPriceInt) # curSerieDict[\"经销商参考价\"] = cityDealerPriceInt curSerieDict[\"厂商指导价\"] = msrpPriceInt return curSerieDict 详见： 【已解决】写Python爬虫爬取汽车之家品牌车系车型数据 crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2021-04-14 19:50:09 "},"appendix/":{"url":"appendix/","title":"附录","keywords":"","body":"附录 下面列出相关参考资料。 crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2021-04-14 19:42:48 "},"appendix/reference.html":{"url":"appendix/reference.html","title":"参考资料","keywords":"","body":"参考资料 【已解决】PySpider中获取PyQuery获取到节点的子元素 【已解决】写Python爬虫爬取汽车之家品牌车系车型数据 【已解决】PySpider中PyQuery选择div后面的第二个span元素 CSS Selectors Reference CSS selectors - CSS: Cascading Style Sheets | MDN CSS 选择器参考手册 crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2021-04-14 19:42:55 "}}